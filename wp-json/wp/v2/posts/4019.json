{"id":4019,"date":"2023-07-24T09:46:02","date_gmt":"2023-07-24T04:16:02","guid":{"rendered":"https:\/\/newwebsite.nestdigital.com\/?p=4019"},"modified":"2023-08-05T16:11:05","modified_gmt":"2023-08-05T10:41:05","slug":"a-brief-overview-of-apache-hadoop","status":"publish","type":"post","link":"https:\/\/nestdigital.com\/blogs\/a-brief-overview-of-apache-hadoop\/","title":{"rendered":"A brief overview of Apache Hadoop"},"content":{"rendered":"\n<p id=\"501b\">Unlike the relational database era, recent times have revolutionized the end-to-end data pipeline from data generation, data collection, data ingestion, data storage, data processing, data analytics and the cherry on top, machine learning and data science. The first platform for handling large volumes of data with respect to distributed storage and parallel processing was Apache Hadoop (It is very popular now as well). In this short blog, I will try to paint a high-level picture of Apache Hadoop and the most popular integrations.<\/p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"e388\">Let\u2019s look at some history<\/h2>\n\n\n\n<p id=\"aa32\">Hadoop was created by&nbsp;<a href=\"https:\/\/www.linkedin.com\/in\/cutting\/\" rel=\"noreferrer noopener\" target=\"_blank\">Doug Cutting<\/a>&nbsp;and&nbsp;<a href=\"https:\/\/www.linkedin.com\/in\/michaelcafarella\" rel=\"noreferrer noopener\" target=\"_blank\">Mike Cafarella<\/a>&nbsp;in 2005 based on Google File System paper that was published in October 2003. It was originally developed to support Apache Nutch (an open-source web crawler product) and later moved to a separate sub-project. Doug, who was working at Yahoo! at the time named the project after his son\u2019s toy elephant \ud83d\ude42 Later in 2008, Hadoop was open sourced and The Apache Software Foundation (ASF) made Hadoop available to the public in November 2012<\/p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"c5d2\">What makes Hadoop special<\/h2>\n\n\n\n<p id=\"851f\">The core idea behind Hadoop was to process large volumes of data in a distributed way using&nbsp;<strong>commodity hardware<\/strong>. It was also an assumption that hardware failures can occur frequently and the software should be capable of handling these and providing fault tolerance. Commodity does not mean cheap, non-usable hardware. It just means that it is relatively inexpensive, widely available and basically interchangeable with other hardware of its type. Unlike purpose-built hardware designed for a specific IT function, commodity hardware can perform many different functions (same hardware can be used to run a web-server, a Linux server, an FTP server, etc.)<\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/miro.medium.com\/v2\/resize:fit:826\/1*3yZYTyuiZ4lRikxl3Zj0sg.jpeg\" alt=\"\"\/><figcaption class=\"wp-element-caption\">Image 1 : Core modules of Apache Hadoop<\/figcaption><\/figure>\n\n\n\n<p id=\"460e\">Hadoop can be configured to run on a single server (for development and testing purposes) and on full scale multi node setup (production scenarios). In any case, the core components for Hadoop are<\/p>\n\n\n\n<p id=\"ea18\"><strong>Hadoop Distributed File System (HDFS)<br><\/strong>HDFS is a highly fault-tolerant distributed file system and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets<br>HDFS has a master slave architecture with a NameNode (master) and number of DataNodes (slaves). HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks (128MB sized) and these blocks are stored in DataNodes. NameNode manages the metadata which is the mapping of blocks to the DataNodes. For fault tolerance, a single file block will have multiple copies (default 3) and they will be stored in multiple DataNodes. Let\u2019s look how a read and write happens in HDFS at high-level<\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/miro.medium.com\/v2\/resize:fit:1400\/1*As-CqGdNktpsoF1r5nM9XA.jpeg\" alt=\"\"\/><figcaption class=\"wp-element-caption\">Image 2 : Read and Write flow in HDFS<\/figcaption><\/figure>\n\n\n\n<p id=\"9815\"><strong>Hadoop MapReduce<br><\/strong>It is a framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.&nbsp;<br>MapReduce application consists of three different tasks. A&nbsp;<em>mapper<\/em>&nbsp;task, a&nbsp;<em>reducer<\/em>&nbsp;task and an optional&nbsp;<em>combiner&nbsp;<\/em>task. The mapper task processes the input file by splitting it into independent chunks and processes them in parallel. The number of mappers depends on the number of&nbsp;<em>InputSplits.&nbsp;<\/em>InputSplit is the smallest data chunk that can be processed independently, which will be a multiplier of HDFS data block (128MB). The reducer takes the outputs of all mapper tasks, sorts the results and then generates the final output. The combiner task is known as a semi-reducer which does a small combine operation in the mapper side itself. This is done to reduce the data volume before the reducer picks it up.<\/p>\n\n\n\n<p id=\"6d96\"><strong>Hadoop YARN (Yet Another Resource Negotiator)<br><\/strong>Interesting name, isn\u2019t it? YARN takes care of the resource management in Hadoop cluster. Any application that tries to run on Hadoop cluster can get the required resources (CPU cores and memory) from YARN.&nbsp;<br>The smallest unit of resource that YARN can allocate is known as a&nbsp;<em>container.&nbsp;<\/em>Container is similar to an actual server. Both have a number of CPU cores, and a finite memory. But the server is physical and container is logical, meaning there can be many containers allocated in a single server. The memory and CPU allocated to a single container is based on YARN configuration properties.&nbsp;<br>Lastly, let\u2019s look quickly at the core services in YARN<\/p>\n\n\n\n<p id=\"6a75\"><em>Resource Manager (RM):&nbsp;<\/em>Central authority who allocates containers for all applications<em><br>Node Manager (NM):&nbsp;<\/em>Service running on each server on the cluster and responsible for containers, monitoring their resource usage and reporting the same to Resource Manager<br><em>Application Master (AM):&nbsp;<\/em>A per application service which negotiates containers from RM for that specific application. AM itself runs as a container<\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/miro.medium.com\/v2\/resize:fit:1400\/1*zxolfwu0s1WS1CnTpokPbw.jpeg\" alt=\"\"\/><figcaption class=\"wp-element-caption\">Image 3 : Overview of application resource allocation by YARN<\/figcaption><\/figure>\n\n\n\n<p id=\"7f25\"><strong>Hadoop Commons and Hadoop Ozone<br><\/strong>Hadoop Common provides a set of services across libraries and utilities to support the other Hadoop modules.<br>Ozone is a scalable, redundant, and distributed object store (Such as AWS S3 or Azure Blob storage) for Hadoop which can scale to billions of objects of varying sizes.<\/p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"fe20\">The ecosystem of rich integrations<\/h2>\n\n\n\n<p id=\"c3f1\">So far, the focus has been on the core components. Let\u2019s shift the focus a little bit to look at some of the very popular integrations runs on top of Hadoop which elevates Hadoop to an enterprise data platform<\/p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https:\/\/miro.medium.com\/v2\/resize:fit:1400\/1*pqHX4r5vmhjaKkV3R5nxpQ.jpeg\" alt=\"\"\/><figcaption class=\"wp-element-caption\">Image 4 : Hadoop and popular integrations<\/figcaption><\/figure>\n\n\n\n<p id=\"0e1f\"><strong>Data movement<\/strong><br><em>Apache Flume:<\/em>&nbsp;A distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data from a variety of sources to HDFS<br><em>Apache Sqoop:&nbsp;<\/em>Data<em>&nbsp;<\/em>importing and exporting service for structured data to and from HDFS. Typically, the input and output sources will be RDBMS systems<br><em>Apache Storm:<\/em>&nbsp;A distributed real-time computation system. Apache Storm makes it easy to reliably process unbounded streams of data, a.k.a real-time processing<\/p>\n\n\n\n<p id=\"4455\"><strong>Data processing<\/strong><br><em>Hive:<\/em>&nbsp;Hive is data warehouse software that facilitates reading, writing, and managing large datasets residing in distributed storage (such as HDFS) using SQL programming<br><em>Pig:<\/em>&nbsp;Pig is data data processing framework which is powered by a proprietary scripting language (Pig Latin) for reading, writing and processing data stored in HDFS<br><em>Tez:&nbsp;<\/em>Apache Tez is a framework comparable to MapReduce. It provides better performance than MR as intermediate results will not be stored on disk, rather in memory. In addition to this vectorization is also used (processing bulk of rows rather than single row at a time)<\/p>\n\n\n\n<p id=\"34e5\">Hive and Pig make use of either MR or Tez for running data processing jobs<\/p>\n\n\n\n<p id=\"4d1e\"><strong>Security<br><\/strong><em>Kerberos:&nbsp;<\/em>In on-premise Hadoop cluster setup, Kerberos is used as the authentication mechanism within the cluster components and between external services and Hadoop<br><em>Ranger:&nbsp;<\/em>Ranger the service for authorization across all Hadoop components. Ranger enables security policies varying from directory, file, database and table access as well as fine grained access on tables<\/p>\n\n\n\n<p id=\"0fa2\"><strong>Oozie&nbsp;<\/strong>It is a workflow scheduler running on Hadoop platform. Oozie is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box<\/p>\n\n\n\n<p id=\"defb\"><strong>Spark&nbsp;<\/strong>An in-memory computation framework which provides batch and real-time data processing capabilities along with machine learning and graph data processing capabilities<\/p>\n\n\n\n<p id=\"d849\"><strong>Data stores &amp; Query engines&nbsp;<br><\/strong><em>Impala:<\/em><strong>&nbsp;<\/strong>A massively parallel processing, highly performant SQL query engine with in-memory processing capabilities to provide very low latency results on large data volumes<br><em>Druid:<\/em>&nbsp;A column oriented, distributed data store whose core design combines ideas from&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Data_warehouse\" rel=\"noreferrer noopener\" target=\"_blank\">data warehouses<\/a>,&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Time_series_database\" rel=\"noreferrer noopener\" target=\"_blank\">timeseries databases<\/a>, and&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Full-text_search\" rel=\"noreferrer noopener\" target=\"_blank\">search systems<\/a>&nbsp;to create a high-performance real-time analytics database<br><em>HBase &amp; Phoenix:<\/em><strong>&nbsp;<\/strong>HBase is a NoSQL, distributed column-oriented data store which provides random read\/write access capabilities on bigdata.&nbsp;<strong><br><\/strong>Phoenix is an SQL query engine that runs on-top of HBase which provides OLTP capabilities to perform various types of querying patterns<\/p>\n\n\n\n<p id=\"8e1d\"><strong>File formats<br><\/strong><em>Avro:&nbsp;<\/em>It is a row-oriented data serialization format which uses JSON to define the schema and creates compact binary data when serialized. It supports full schema evolution<em><br>ORC (Optimized Row Columnar):&nbsp;<\/em>It is an optimized column-oriented data storage format mainly used in Hive. It also enables Hive to have transactional capabilities similar to OLTP database<em><br>Parquet:&nbsp;<\/em>Similar to ORC, it is another column-oriented data storage format which is widely used with various data processing platforms such as Apache Spark<\/p>\n\n\n\n<p id=\"4241\"><strong>Zeppelin<\/strong>&nbsp;A web-based notebook which brings data exploration, visualization, sharing and collaboration feature to Hadoop used by data analysts, data scientists, etc. for data exploration<\/p>\n\n\n\n<p id=\"17fb\"><strong>Superset<\/strong>&nbsp;A powerful web-based data visualization platform which provides rich visualization and dashboarding capabilities with connectivity to Hive, Druid, HBase and Impala and other data stores<\/p>\n\n\n\n<p id=\"9c0f\"><strong>Ambari<\/strong>&nbsp;Central management service for Hadoop which enables provisioning, managing, and monitoring Apache Hadoop clusters<\/p>\n\n\n\n<p id=\"a26c\"><strong>Atlas<\/strong>&nbsp;An open-source metadata management and governance system designed to help enterprises to easily find, organize, and manage data assets in Hadoop platform<\/p>\n\n\n\n<p id=\"10d8\"><strong>Zookeeper<\/strong>&nbsp;It is a distributed coordination service that helps to manage a large number of hosts such as Hadoop cluster<\/p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"b8df\">Hadoop enterprise<\/h2>\n\n\n\n<p id=\"0201\">CDP (Cloudera Data Platform)<br>AWS EMR (Elastic MapReduce)<br>Azure HDInsight<br>Google Cloud Dataproc<\/p>\n\n\n\n<h2 class=\"wp-block-heading\" id=\"96ef\">References<\/h2>\n\n\n\n<p id=\"bc93\"><a href=\"https:\/\/hadoop.apache.org\/docs\/stable\/\" rel=\"noreferrer noopener\" target=\"_blank\">Hadoop \u2014 Apache Hadoop 3.3.3<\/a><\/p>\n\n\n\n<blockquote class=\"wp-block-quote\">\n<p id=\"8b63\">Manu Mukundan, Data Architect, TE, Nest Digital<\/p>\n<\/blockquote>\n","protected":false},"excerpt":{"rendered":"<p>Unlike the relational database era, recent times have revolutionized the end-to-end data pipeline from data generation, data collection, data ingestion, data storage, data processing, data analytics and the cherry on top, machine learning and data science. The first platform for handling large volumes of data with respect to distributed storage and parallel processing was Apache [&hellip;]<\/p>\n","protected":false},"author":2,"featured_media":4477,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"inline_featured_image":false,"footnotes":"","wds_primary_category":16},"categories":[16],"tags":[36],"blocksy_meta":"","_links":{"self":[{"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/posts\/4019"}],"collection":[{"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/comments?post=4019"}],"version-history":[{"count":0,"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/posts\/4019\/revisions"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/media\/4477"}],"wp:attachment":[{"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/media?parent=4019"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/categories?post=4019"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/nestdigital.com\/wp-json\/wp\/v2\/tags?post=4019"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}